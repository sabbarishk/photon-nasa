# {{ title }}
# NASA Dataset Analysis & Visualization
# Dataset: {{ dataset_url }}
# Variable: {{ variable }}

import pandas as pd
import numpy as np
import matplotlib
matplotlib.use('Agg')
import matplotlib.pyplot as plt
import warnings
warnings.filterwarnings('ignore')

plt.style.use('seaborn-v0_8-darkgrid')
plt.rcParams['figure.figsize'] = (14, 10)
plt.rcParams['font.size'] = 11

# ============================================================
# STEP 1: LOAD AND VALIDATE DATA
# ============================================================
print("üìä Loading NASA dataset...")

import urllib.request
import io

def load_nasa_csv(url, variable_name):
    """Smart NASA CSV loader with validation."""
    
    # Step 1: Fetch raw content and validate it's actually CSV
    try:
        req = urllib.request.Request(url, headers={'User-Agent': 'Mozilla/5.0'})
        with urllib.request.urlopen(req, timeout=30) as response:
            content_type = response.headers.get('Content-Type', '')
            raw_bytes = response.read()
            raw_text = raw_bytes.decode('utf-8', errors='replace')
    except Exception as e:
        raise ValueError(f"‚ùå Cannot fetch URL: {url}\nError: {e}")
    
    # Step 2: Check if response is HTML (wrong URL)
    stripped = raw_text.strip()
    if stripped.startswith('<!') or stripped.startswith('<html') or stripped.startswith('<HTML'):
        raise ValueError(
            f"‚ùå URL returned an HTML page, not CSV data.\n"
            f"URL: {url}\n"
            f"This dataset may require authentication or the direct download URL is different.\n"
            f"Try finding the direct .csv download link from the dataset page."
        )
    
    # Step 3: Check for XML/JSON
    if stripped.startswith('<?xml') or stripped.startswith('<feed') or stripped.startswith('{'):
        raise ValueError(
            f"‚ùå URL returned XML/JSON, not CSV data.\n"
            f"URL: {url}\n"
            f"Please use the direct CSV download URL."
        )
    
    print(f"‚úÖ Content looks like CSV data ({len(raw_text):,} bytes)")
    
    # Step 4: Detect header rows (NASA CSVs often have comment lines)
    lines = raw_text.split('\n')
    skiprows = 0
    for i, line in enumerate(lines[:30]):
        line = line.strip()
        # Skip comment lines, empty lines, and metadata lines
        if (line.startswith('#') or 
            line.startswith('%') or 
            line == '' or
            (len(line) > 0 and not any(c.isdigit() or c == ',' or c == '\t' for c in line[:20]))):
            skiprows = i + 1
        else:
            break
    
    if skiprows > 0:
        print(f"‚ÑπÔ∏è  Skipping {skiprows} header/comment rows")
    
    # Step 5: Try multiple CSV parsing strategies
    strategies = [
        {"sep": ",",  "skiprows": skiprows,     "on_bad_lines": "skip"},
        {"sep": "\t", "skiprows": skiprows,     "on_bad_lines": "skip"},
        {"sep": ",",  "skiprows": skiprows + 1, "on_bad_lines": "skip"},
        {"sep": None, "skiprows": skiprows,     "on_bad_lines": "skip", "engine": "python"},
        {"sep": ",",  "skiprows": 0,            "on_bad_lines": "skip"},
    ]
    
    df = None
    for i, params in enumerate(strategies):
        try:
            test_df = pd.read_csv(io.StringIO(raw_text), **params)
            # Validate: must have >1 row, >1 col, and not HTML
            if len(test_df) > 1 and len(test_df.columns) > 0:
                col_names = [str(c) for c in test_df.columns]
                if not any('DOCTYPE' in c or '<html' in c.lower() for c in col_names):
                    df = test_df
                    print(f"‚úÖ Strategy {i+1} succeeded: {df.shape[0]} rows √ó {df.shape[1]} cols")
                    break
        except Exception:
            continue
    
    if df is None:
        raise ValueError(
            f"‚ùå Could not parse CSV data from URL.\n"
            f"First 200 chars of content:\n{raw_text[:200]}"
        )
    
    # Step 6: Clean column names
    df.columns = [str(c).strip().replace('\r', '') for c in df.columns]
    
    # Step 7: Handle GISS-style MultiIndex (column names are in first data row)
    first_row = df.iloc[0].astype(str).tolist()
    if all(v.replace('.','').replace('-','').replace('*','').isalpha() for v in first_row[:5] if v not in ['nan','']):
        print("‚ÑπÔ∏è  Detected header row in data ‚Äî extracting column names...")
        df.columns = first_row
        df = df.iloc[1:].reset_index(drop=True)
        print(f"‚úÖ Columns extracted: {list(df.columns[:8])}...")
    
    # Step 8: Convert numeric columns
    for col in df.columns:
        df[col] = pd.to_numeric(df[col].astype(str).str.strip().str.replace('***', 'nan'), errors='ignore')
    
    # Step 9: Find the variable column
    target_col = None
    # Exact match first
    if variable_name in df.columns:
        target_col = variable_name
    else:
        # Case-insensitive partial match
        var_lower = variable_name.lower()
        for col in df.columns:
            if var_lower in str(col).lower() or str(col).lower() in var_lower:
                target_col = col
                break
    
    if target_col is None:
        # Use first numeric column
        numeric_cols = df.select_dtypes(include=[np.number]).columns.tolist()
        if numeric_cols:
            target_col = numeric_cols[0]
            print(f"‚ö†Ô∏è  Variable '{variable_name}' not found. Using '{target_col}' instead.")
            print(f"    Available columns: {list(df.columns[:10])}")
        else:
            raise ValueError(
                f"‚ùå No numeric columns found.\n"
                f"Available columns: {list(df.columns)}\n"
                f"Please specify one of these as your variable name."
            )
    else:
        print(f"‚úÖ Using column: '{target_col}'")
    
    return df, target_col

# Load the data
try:
    df, target_col = load_nasa_csv(url, variable_name)
except ValueError as e:
    print(str(e))
    raise

print(f"\nüìã Dataset shape: {df.shape}")
print(f"üìå Target variable: {target_col}")
print(f"üìä Sample values: {df[target_col].dropna().head(5).tolist()}")

# Detect time / year column
time_col = None
for c in df.columns:
    cl = c.lower()
    if any(k in cl for k in ['year', 'date', 'time', 'month', 'day']):
        time_col = c
        break

# Convert all non-time columns to numeric
for c in df.columns:
    df[c] = pd.to_numeric(df[c], errors='coerce')

df.dropna(how='all', subset=[c for c in df.columns if c != time_col], inplace=True)
numeric_cols = df.select_dtypes(include=[np.number]).columns.tolist()

variable = "{{ variable }}".strip()
if variable in df.columns:
    data_col = variable
elif len(numeric_cols) > 0:
    candidates = [c for c in numeric_cols if c != time_col]
    data_col = candidates[0] if candidates else numeric_cols[0]
    print(f"‚ö†Ô∏è  Variable '{variable}' not found ‚Äì using '{data_col}'")
else:
    print("‚ùå No numeric data found.")
    raise SystemExit(1)

# Build x/y arrays
if time_col and time_col in df.columns:
    clean = df[[time_col, data_col]].dropna()
    x_vals = clean[time_col].values.astype(float)
    y_vals = clean[data_col].values.astype(float)
    x_label = time_col
else:
    clean = df[[data_col]].dropna()
    y_vals = clean[data_col].values.astype(float)
    x_vals = np.arange(len(y_vals), dtype=float)
    x_label = "Index"

print(f"\n‚úÖ {len(y_vals)} data points for '{data_col}'")
print(f"   Range: {y_vals.min():.4f} ‚Üí {y_vals.max():.4f}  |  Mean: {y_vals.mean():.4f}  |  Std: {y_vals.std():.4f}")

# ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ
# STEP 3: Visualise
# ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ
fig, axes = plt.subplots(2, 3, figsize=(18, 11))
fig.suptitle("{{ title }}", fontsize=17, fontweight='bold', y=0.99)

# Plot 1: Time series
ax = axes[0, 0]
ax.plot(x_vals, y_vals, lw=1.8, color='#2E86AB', alpha=0.85, marker='o', markersize=2.5)
if len(y_vals) > 2:
    z = np.polyfit(x_vals, y_vals, 1)
    p = np.poly1d(z)
    ax.plot(x_vals, p(x_vals), '--', color='#C73E1D', lw=2, label=f'Trend: {z[0]:+.4f}/unit')
    ax.legend(fontsize=9)
ax.set_title(f"{data_col} over {x_label}", fontweight='bold')
ax.set_xlabel(x_label); ax.set_ylabel(data_col)
ax.grid(True, alpha=0.3)

# Plot 2: Histogram
ax = axes[0, 1]
ax.hist(y_vals, bins=min(30, max(10, len(y_vals)//5)),
        color='#F18F01', edgecolor='white', alpha=0.8)
try:
    from scipy.stats import gaussian_kde
    kde_x = np.linspace(y_vals.min(), y_vals.max(), 300)
    kde = gaussian_kde(y_vals)
    ax2_twin = ax.twinx()
    ax2_twin.plot(kde_x, kde(kde_x), color='#C73E1D', lw=2, label='KDE')
    ax2_twin.set_ylabel('Density')
    ax2_twin.legend(loc='upper right', fontsize=9)
except Exception:
    pass
ax.set_title("Distribution", fontweight='bold')
ax.set_xlabel(data_col); ax.set_ylabel("Count")
ax.grid(True, alpha=0.3, axis='y')

# Plot 3: Box plot
ax = axes[0, 2]
ax.boxplot(y_vals, vert=True, patch_artist=True,
           boxprops=dict(facecolor='#84DCC6', alpha=0.7),
           medianprops=dict(color='#C73E1D', lw=2))
stats_txt = (f"Mean:   {np.mean(y_vals):.3f}\n"
             f"Median: {np.median(y_vals):.3f}\n"
             f"Std:    {np.std(y_vals):.3f}\n"
             f"Min:    {np.min(y_vals):.3f}\n"
             f"Max:    {np.max(y_vals):.3f}")
ax.text(1.35, np.median(y_vals), stats_txt, fontsize=9,
        va='center', bbox=dict(boxstyle='round', facecolor='wheat', alpha=0.6))
ax.set_title("Box Plot", fontweight='bold')
ax.set_ylabel(data_col); ax.grid(True, alpha=0.3, axis='y')

# Plot 4: Rolling mean
ax = axes[1, 0]
ax.plot(x_vals, y_vals, 'o-', alpha=0.25, ms=2, color='gray', label='Raw')
if len(y_vals) > 10:
    win = max(5, len(y_vals) // 15)
    roll = pd.Series(y_vals).rolling(win, center=True)
    rm = roll.mean().values
    rs = roll.std().values
    ax.plot(x_vals, rm, lw=2, color='#2E86AB', label=f'{win}-pt avg')
    ax.fill_between(x_vals, rm - rs, rm + rs, alpha=0.2, color='#F18F01', label='¬±1œÉ')
ax.legend(fontsize=9)
ax.set_title("Rolling Statistics", fontweight='bold')
ax.set_xlabel(x_label); ax.set_ylabel(data_col); ax.grid(True, alpha=0.3)

# Plot 5: Anomaly bars
ax = axes[1, 1]
mean_val = np.mean(y_vals)
anomaly = y_vals - mean_val
bar_colors = ['#C73E1D' if a > 0 else '#2E86AB' for a in anomaly]
bar_width = max(0.6, (x_vals[-1] - x_vals[0]) / max(len(x_vals), 1) * 0.9) if len(x_vals) > 1 else 0.6
ax.bar(x_vals, anomaly, color=bar_colors, alpha=0.7, width=bar_width)
ax.axhline(0, color='black', lw=0.8)
ax.set_title(f"Anomaly (baseline = {mean_val:.3f})", fontweight='bold')
ax.set_xlabel(x_label); ax.set_ylabel(f"{data_col} anomaly"); ax.grid(True, alpha=0.3, axis='y')

# Plot 6: Correlation heatmap OR period-change bars
ax = axes[1, 2]
num_candidates = [c for c in numeric_cols if c != time_col]
if len(num_candidates) >= 2:
    try:
        import seaborn as sns
        corr = df[num_candidates[:8]].corr()
        sns.heatmap(corr, annot=True, fmt='.2f', cmap='coolwarm', center=0,
                    square=True, ax=ax, cbar_kws={'shrink': 0.8}, annot_kws={'size': 8})
        ax.set_title("Correlation Matrix", fontweight='bold')
    except Exception:
        ax.axis('off')
        ax.text(0.5, 0.5, f"Dataset: {len(y_vals)} points\n{data_col}\nMean: {mean_val:.4f}",
                ha='center', va='center', fontsize=12, transform=ax.transAxes)
        ax.set_title("Summary", fontweight='bold')
elif len(y_vals) > 2:
    dy = np.diff(y_vals)
    ax.bar(x_vals[1:], dy, color=['#84DCC6' if v >= 0 else '#C73E1D' for v in dy], alpha=0.7)
    ax.axhline(0, color='black', lw=0.8)
    ax.set_title(f"Period-over-Period Œî{data_col}", fontweight='bold')
    ax.set_xlabel(x_label); ax.set_ylabel(f"Œî{data_col}"); ax.grid(True, alpha=0.3, axis='y')
else:
    ax.axis('off')
    ax.set_title("Summary", fontweight='bold')

plt.tight_layout()
plt.show()

print("\n‚úÖ Analysis complete!")
print(f"   Dataset  : {{ dataset_url }}")
print(f"   Variable : {data_col}")
print(f"   Points   : {len(y_vals)}")
print(f"   Mean     : {np.mean(y_vals):.4f}")
print(f"   Std      : {np.std(y_vals):.4f}")
print(f"   Range    : [{np.min(y_vals):.4f}, {np.max(y_vals):.4f}]")
